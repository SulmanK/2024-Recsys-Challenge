{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a295dea",
   "metadata": {},
   "source": [
    "# 2024 Recsys Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92703771",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3934a3",
   "metadata": {},
   "source": [
    "This year's challenge focuses on online news recommendation, addressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. The challenge will delve into the unique aspects of news recommendation, including modeling user preferences based on implicit behavior, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news items. Furthermore, our challenge embraces the normative complexities, involving investigating the effects of recommender systems on the news flow and whether they resonate with editorial values. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760b2a",
   "metadata": {},
   "source": [
    "## Challenge Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237298c2-2986-4bba-bcbd-5ff27e361919",
   "metadata": {},
   "source": [
    "The Ekstra Bladet RecSys Challenge aims to predict which article a user will click on from a list of articles that were seen during a specific impression. Utilizing the user's click history, session details (like time and device used), and personal metadata (including gender and age), along with a list of candidate news articles listed in an impression log, the challenge's objective is to rank the candidate articles based on the user's personal preferences. This involves developing models that encapsulate both the users and the articles through their content and the users' interests. The models are to estimate the likelihood of a user clicking on each article by evaluating the compatibility between the article's content and the user's preferences. The articles are ranked based on these likelihood scores, and the precision of these rankings is measured against the actual selections made by users. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788cf29",
   "metadata": {},
   "source": [
    "## Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766e23f",
   "metadata": {},
   "source": [
    "The Ekstra Bladet News Recommendation Dataset (EB-NeRD) was created to support advancements in news recommendation research. It was collected from user behavior logs at Ekstra Bladet. We collected behavior logs from active users during the 6 weeks from April 27 to June 8, 2023. This timeframe was selected to avoid major events, e.g., holidays or elections, that could trigger atypical behavior at Ekstra Bladet. The active users were defined as users who had at least 5 and at most 1,000 news click records in a three-week period from May 18 to June 8, 2023. To protect user privacy, every user was delinked from the production system when securely hashed into an anonymized ID using one-time salt mapping. Alongside, we provide Danish news articles published by Ekstra Bladet. Each article is enriched with textual context features such as title, abstract, body, categories, among others. Furthermore, we provide features that have been generated by proprietary models, including topics, named entity recognition (NER), and article embeddings [2]\n",
    "\n",
    "For more information on the [dataset](https://recsys.eb.dk/dataset/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb08b9d",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [RecySys Challenge 2024 Logistics](https://recsys.eb.dk/)\n",
    "\n",
    "[2] [Ekstra Bladet News Recommendation Dataset](https://recsys.eb.dk/dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e0d1c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97e6cd",
   "metadata": {},
   "source": [
    "### Notebook Organization\n",
    "### This purpose of this notebook is for EDA only. \n",
    "\n",
    "- Logistics\n",
    "- EDA \n",
    "    - Data Preprocessing\n",
    "    - Functions\n",
    "        - Plot Functions\n",
    "        - Feature Functions\n",
    "            - Article\n",
    "            - User\n",
    "            - Topic\n",
    "            - Activity\n",
    "    - Feature Analysis\n",
    "        - Overall Feature Analysis\n",
    "        - Article\n",
    "        - User\n",
    "        - Session\n",
    "        - Topic\n",
    "        - Devices\n",
    "        - If subscriber\n",
    "        - Gender\n",
    "        - Age\n",
    "        - Postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e806296",
   "metadata": {},
   "source": [
    "We need to establish specific metrics and analyze how different features impact those metrics. Our platform generates revenue through both subscriptions and advertisements. User engagement is crucial because the more time users spend reading new articles, the greater our advertisement revenue. With this in focus, let's start with exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927be23c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f05b58-dde3-4d5f-87a2-f20ad71cd62b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4caea",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d0f39",
   "metadata": {},
   "source": [
    "Let's import our packages used for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651173e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616fa6a4-e47a-4116-82f5-896d6893a00e",
   "metadata": {},
   "source": [
    "Load in the three separate data sources of the dataset:\n",
    "\n",
    "**Articles**: Detailed information of news articles.[*](https://recsys.eb.dk/dataset/#articles)\n",
    "\n",
    "**Behaviors**: Impression Logs. [*](https://recsys.eb.dk/dataset/#behaviors)\n",
    "\n",
    "**History**: Click histories of users. [*](https://recsys.eb.dk/dataset/#history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a57c46-4e08-4299-8a69-12d45e274b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in various dataframes\n",
    "# Articles\n",
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/train/history.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8b12076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in various dataframes\n",
    "# Articles\n",
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/validation/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/validation/history.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d447c",
   "metadata": {},
   "source": [
    "What feature can we join the data sources on?\n",
    "\n",
    "- Articles & Behavior: Article ID\n",
    "\n",
    "- History & Behavior: User ID\n",
    "\n",
    "Before we can join, we need to modify the behavior['article_ids_clicked'] column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5eaa9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datatype of column first\n",
    "df_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if isinstance(x, str) else int(x) if not np.isnan(x) else x)\n",
    "\n",
    "# Join bevhaiors to article\n",
    "df = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n",
    "\n",
    "# Join bevhaiors to history\n",
    "df = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n",
    "\n",
    "# Drop all other dataframes from me\n",
    "df_bev = []\n",
    "df_his = []\n",
    "df_art = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87a8d8",
   "metadata": {},
   "source": [
    "More preprocessing needed before we can begin further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3cc9be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_(x):\n",
    "    \"\"\" \n",
    "    Changes the device input from a int to a str\n",
    "    Keyword arguments:\n",
    "        x -- int\n",
    "    Output:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if x == 1:\n",
    "        return 'Desktop'\n",
    "    elif x == 2:\n",
    "        return 'Mobile'\n",
    "    else:\n",
    "        return 'Tablet'\n",
    "\n",
    "def gender_(x):\n",
    "    \"\"\" \n",
    "    Changes the gender input from a float to a str\n",
    "    Keyword arguments:\n",
    "        x -- float\n",
    "    Output:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if x == 0.0:\n",
    "        return 'Male'\n",
    "    elif x == 1.0:\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def postcodes_(x):\n",
    "    \"\"\" \n",
    "    Changes the postcodes input from a float to a str\n",
    "    Keyword arguments:\n",
    "        x -- float\n",
    "    Output:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if x == 0.0:\n",
    "        return 'Metropolitan'\n",
    "    elif x == 1.0:\n",
    "        return 'Rural District'\n",
    "\n",
    "    elif x == 2.0:\n",
    "        return 'Municipality'\n",
    "\n",
    "    elif x == 3.0:\n",
    "        return 'Provincial'\n",
    "\n",
    "    elif x == 4.0:\n",
    "        return 'Big City'\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5796b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df.dropna(subset=['article_id'], inplace=True)\n",
    "\n",
    "# Change article IDs into int\n",
    "df['article_id'] = df['article_id'].apply(lambda x: int(x))\n",
    "df['article_id'] = df['article_id'].astype(np.int64)\n",
    "\n",
    "# Change age from int to string\n",
    "df['device_type'] = df['device_type'].apply(lambda x: device_(x))\n",
    "\n",
    "# Change genders from float to string\n",
    "df['gender'] = df['gender'].apply(lambda x: gender_(x))\n",
    "\n",
    "# Change age to str it's a range\n",
    "df['age'] = df['age'].astype('Int64')\n",
    "df['age'] = df['age'].astype(str)\n",
    "df['age'] = df['age'].apply(\n",
    "    lambda x: x if x == '<NA>' else x + ' - ' + x[0] + '9')\n",
    "\n",
    "\n",
    "# Change postcodes from int to str\n",
    "df['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3427472",
   "metadata": {},
   "source": [
    "Next section will be on all the helper functions used in this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2493e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a4266",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f88f9",
   "metadata": {},
   "source": [
    "We will explore news recommendation systems, starting from first principles and progressing to state-of-the-art approaches. This will include exploring content-based approaches and hybrid-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a399c",
   "metadata": {},
   "source": [
    "## Content-Based Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa11902",
   "metadata": {},
   "source": [
    "Our first approach will be utilizing TF-IDF vectorization because of its simplicity.\n",
    "\n",
    "In conjuction with TF-IDF, we need to utilize a similarity method to compare documents such as cosine similarity or FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff8a0d",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82f02a",
   "metadata": {},
   "source": [
    "Cosine similarity is a measure used to determine the similarity between two non-zero vectors in a multi-dimensional space. It calculates the cosine of the angle between the vectors, with values ranging from -1 to 1. In the context of NLP and recommender systems, it is commonly used to compare TF-IDF or word embeddings to evaluate the similarity between documents or items. Setting an appropriate threshold value is crucial for determining the significance of similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# Load in training data\n",
    "# Articles\n",
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/train/history.parquet\")\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert datatype of column first\n",
    "df_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if isinstance(x, str) else int(x) if not np.isnan(x) else x)\n",
    "\n",
    "# Join bevhaiors to article\n",
    "df = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n",
    "\n",
    "# Join bevhaiors to history\n",
    "df = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n",
    "\n",
    "# Drop all other dataframes from me\n",
    "df_bev = []\n",
    "df_his = []\n",
    "df_art = []\n",
    "df.dropna(subset=['article_id'], inplace=True)\n",
    "\n",
    "# Change article IDs into int\n",
    "df['article_id'] = df['article_id'].apply(lambda x: int(x))\n",
    "df['article_id'] = df['article_id'].astype(np.int64)\n",
    "\n",
    "# Change age from int to string\n",
    "df['device_type'] = df['device_type'].apply(lambda x: device_(x))\n",
    "\n",
    "# Change genders from float to string\n",
    "df['gender'] = df['gender'].apply(lambda x: gender_(x))\n",
    "\n",
    "# Change age to str it's a range\n",
    "df['age'] = df['age'].astype('Int64')\n",
    "df['age'] = df['age'].astype(str)\n",
    "df['age'] = df['age'].apply(\n",
    "    lambda x: x if x == '<NA>' else x + ' - ' + x[0] + '9')\n",
    "\n",
    "\n",
    "# Change postcodes from int to str\n",
    "df['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))\n",
    "\n",
    "# Modeling\n",
    "# Rand Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Merge fields into strings for processing\n",
    "df_art['topics_str'] = df_art['topics'].apply(' '.join)\n",
    "df_art['entity_groups_str'] = df_art['entity_groups'].apply(' '.join)\n",
    "df_art['ner_clusters_str'] = df_art['ner_clusters'].apply(' '.join)\n",
    "\n",
    "# Create a dictionary for quick lookups\n",
    "article_content_dict = {\n",
    "    row['article_id']: f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} \"\n",
    "                       f\"{row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n",
    "    for _, row in df_art.iterrows()\n",
    "}\n",
    "\n",
    "# Fit TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l1')\n",
    "tfidf_matrix_all = vectorizer.fit_transform(article_content_dict.values())\n",
    "article_ids = list(article_content_dict.keys())\n",
    "\n",
    "# Map article IDs to indices in the TF-IDF matrix\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(article_ids)}\n",
    "\n",
    "# Initialize predicted impressions list\n",
    "predicted_impressions = []\n",
    "# Store similarity scores for auc_score\n",
    "similarity_scores = []\n",
    "\n",
    "# Process each row of the user behavior data\n",
    "for i in df_bev.index[0:1000]:  \n",
    "    # Extract the articles viewed by the current user\n",
    "    user_article_history = df_bev.loc[i, 'article_ids_inview']\n",
    "    \n",
    "    # Map article IDs to indices if they exist in the article ID-to-index dictionary\n",
    "    indices = [article_id_to_idx[x] for x in user_article_history if x in article_id_to_idx]\n",
    "    \n",
    "    # If no valid indices are found, append None to predictions and skip further processing\n",
    "    if not indices:\n",
    "        predicted_impressions.append(None)\n",
    "        continue\n",
    "    \n",
    "    # Compute the average TF-IDF vector for the user's article history\n",
    "    user_profile_vector = tfidf_matrix_all[indices].mean(axis=0).A\n",
    "    \n",
    "    highest_similarity = 0\n",
    "    best_imp = None\n",
    "\n",
    "    # Evaluate each article in the user's history for similarity to the user's profile\n",
    "    for imp in user_article_history:\n",
    "        if imp in article_id_to_idx:\n",
    "            # Retrieve the TF-IDF vector for the article\n",
    "            imp_idx = article_id_to_idx[imp]\n",
    "            imp_tfidf_vector = tfidf_matrix_all[imp_idx].A\n",
    "\n",
    "            # Calculate the cosine similarity between the user's profile and the article\n",
    "            similarity = cosine_similarity(user_profile_vector, imp_tfidf_vector.reshape(1, -1))[0, 0]\n",
    "            \n",
    "            # Update the best match if the current similarity is higher\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                best_imp = imp\n",
    "\n",
    "    # If similarity is low, pick a random article; otherwise, use the best match\n",
    "    if highest_similarity <= 0.5:\n",
    "        impression = np.random.choice(user_article_history)\n",
    "        predicted_impressions.append(impression)\n",
    "        # Assign low similarity value for random choice\n",
    "        similarity_scores.append(0)  \n",
    "        print(f\"User {df_bev.loc[i, 'user_id']}: Low similarity, random impression chosen\")\n",
    "    else:\n",
    "        predicted_impressions.append(best_imp)\n",
    "        similarity_scores.append(highest_similarity)\n",
    "        print(f\"User {df_bev.loc[i, 'user_id']}: Best impression {best_imp} with similarity {highest_similarity}\")\n",
    "\n",
    "# Prepare binary labels for AUC\n",
    "actual_impressions = [x[0] for x in df_bev['article_ids_clicked'].values][0:1000]\n",
    "binary_labels = [1 if pred == actual else 0 for pred, actual in zip(predicted_impressions, actual_impressions)]\n",
    "auc_score = sklearn.metrics.roc_auc_score(binary_labels, similarity_scores)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_pred = predicted_impressions\n",
    "y_true = actual_impressions\n",
    "acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37343d90",
   "metadata": {},
   "source": [
    "The model's accuracy is 0.12, lower than a random model's 1/6 (≈0.1667) chance. However, its ROC-AUC score is 0.59, indicating a decent starting point as it outperforms random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c99e9",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af110ab",
   "metadata": {},
   "source": [
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI that provides efficient tools for similarity search and clustering of dense vectors. The central task in FAISS is finding vectors in a database that are closest to a query vector, typically using a similarity measure like cosine similarity or Euclidean distance. Setting an appropriate threshold value is crucial for determining the significance of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "m\n",
    "# Load in training data\n",
    "# Articles\n",
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/train/history.parquet\")\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert datatype of column first\n",
    "df_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if isinstance(x, str) else int(x) if not np.isnan(x) else x)\n",
    "\n",
    "# Join bevhaiors to article\n",
    "df = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n",
    "\n",
    "# Join bevhaiors to history\n",
    "df = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n",
    "\n",
    "# Drop all other dataframes from me\n",
    "df_bev = []\n",
    "df_his = []\n",
    "df_art = []\n",
    "df.dropna(subset=['article_id'], inplace=True)\n",
    "\n",
    "# Change article IDs into int\n",
    "df['article_id'] = df['article_id'].apply(lambda x: int(x))\n",
    "df['article_id'] = df['article_id'].astype(np.int64)\n",
    "\n",
    "# Change age from int to string\n",
    "df['device_type'] = df['device_type'].apply(lambda x: device_(x))\n",
    "\n",
    "# Change genders from float to string\n",
    "df['gender'] = df['gender'].apply(lambda x: gender_(x))\n",
    "\n",
    "# Change age to str it's a range\n",
    "df['age'] = df['age'].astype('Int64')\n",
    "df['age'] = df['age'].astype(str)\n",
    "df['age'] = df['age'].apply(\n",
    "    lambda x: x if x == '<NA>' else x + ' - ' + x[0] + '9')\n",
    "\n",
    "\n",
    "# Change postcodes from int to str\n",
    "df['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))\n",
    "\n",
    "# Modeling\n",
    "# Rand Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Merge fields into strings for processing\n",
    "df_art['topics_str'] = df_art['topics'].apply(' '.join)\n",
    "df_art['entity_groups_str'] = df_art['entity_groups'].apply(' '.join)\n",
    "df_art['ner_clusters_str'] = df_art['ner_clusters'].apply(' '.join)\n",
    "\n",
    "# Create a dictionary for quick lookups\n",
    "article_content_dict = {\n",
    "    row['article_id']: f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} \"\n",
    "                       f\"{row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n",
    "    for _, row in df_art.iterrows()\n",
    "}\n",
    "\n",
    "# Fit TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l1')\n",
    "tfidf_matrix_all = vectorizer.fit_transform(article_content_dict.values())\n",
    "article_ids = list(article_content_dict.keys())\n",
    "\n",
    "# Map article IDs to indices in the TF-IDF matrix\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(article_ids)}\n",
    "\n",
    "# Initialize predicted impressions list\n",
    "predicted_impressions = []\n",
    "# Store similarity scores for auc_score\n",
    "similarity_scores = []\n",
    "\n",
    "# Process each user behavior\n",
    "for i in df_bev.index[0:1000]:\n",
    "    user_article_history = df_bev.loc[i, 'article_ids_inview']\n",
    "    indices = [article_id_to_idx[x] for x in user_article_history if x in article_id_to_idx]\n",
    "\n",
    "    if not indices:\n",
    "        predicted_impressions.append(None)\n",
    "        continue\n",
    "\n",
    "    # Aggregate user history into a single vector\n",
    "    user_profile_vector = tfidf_matrix_reduced[indices].mean(axis=0).reshape(1, -1).astype(np.float32)\n",
    "\n",
    "    # Use FAISS to find nearest neighbors (articles) based on user profile vector\n",
    "    # Search for top 5 nearest neighbors (articles)\n",
    "    D, I = index.search(user_profile_vector, k=10)  \n",
    "\n",
    "    # Get the most similar article based on the nearest neighbor\n",
    "    # The distance (smallest distance = most similar)\n",
    "    highest_similarity = D[0][0]\n",
    "    print(highest_similarity)\n",
    "    # The index of the closest article\n",
    "    best_imp_idx = I[0][0] \n",
    "\n",
    "    # Handle low similarity by selecting a random article\n",
    "    if highest_similarity < 0.00005:\n",
    "        impression = np.random.choice(user_article_history)\n",
    "        predicted_impressions.append(impression)\n",
    "        # Assign low similarity value for random choice\n",
    "        similarity_scores.append(0)  \n",
    "        print(f\"User {df_bev.loc[i, 'user_id']}: Low similarity, random impression chosen\")\n",
    "    else:\n",
    "        best_imp = article_ids[best_imp_idx]  # Retrieve the article ID of the best match\n",
    "        predicted_impressions.append(best_imp)\n",
    "        similarity_scores.append(highest_similarity)  # Store the similarity score\n",
    "        print(f\"User {df_bev.loc[i, 'user_id']}: Best impression {best_imp} with similarity {highest_similarity}\")\n",
    "\n",
    "\n",
    "# Prepare binary labels for AUC\n",
    "actual_impressions = [x[0] for x in df_bev['article_ids_clicked'].values][0:1000]\n",
    "binary_labels = [1 if pred == actual else 0 for pred, actual in zip(predicted_impressions, actual_impressions)]\n",
    "auc_score = sklearn.metrics.roc_auc_score(binary_labels, similarity_scores)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_pred = predicted_impressions\n",
    "y_true = actual_impressions\n",
    "acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5df5ef",
   "metadata": {},
   "source": [
    "The model's accuracy is 0.075, below a random model's 1/6 (≈0.1667) chance, and its ROC-AUC score of 0.330 indicates it performs worse than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337dee18",
   "metadata": {},
   "source": [
    "## Hybrid-Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d7b06",
   "metadata": {},
   "source": [
    "Content-based approaches performed poorly due to the use of TF-IDF vectorization, a bag-of-words model that fails to capture relationships between words. Now, we'll use more state-of-the-art models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487e4f9",
   "metadata": {},
   "source": [
    "#### Procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce316bb1",
   "metadata": {},
   "source": [
    "1) Prepare the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c336cf",
   "metadata": {},
   "source": [
    "2) Run the following script in the fuxcitr_dir directory to train the model on train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_param_tuner.py --config config/DIN_ebnerd_large_x1_tuner_config_01.yaml --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d038127",
   "metadata": {},
   "source": [
    "3) Run the following script in the fuxcitr_dir directory to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ff7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "python submit.py --config config/DIN_ebnerd_large_x1_tuner_config_01 --expid DIN_ebnerd_large_x1_001_1860e41e --gpu 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfc6a5",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea66c5",
   "metadata": {},
   "source": [
    "The model's accuracy is 0.7154, which is much better than our previous model scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9b187",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
