{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a295dea",
   "metadata": {},
   "source": [
    "# 2024 Recsys Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92703771",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3934a3",
   "metadata": {},
   "source": [
    "This year's challenge focuses on online news recommendation, addressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. The challenge will delve into the unique aspects of news recommendation, including modeling user preferences based on implicit behavior, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news items. Furthermore, our challenge embraces the normative complexities, involving investigating the effects of recommender systems on the news flow and whether they resonate with editorial values. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760b2a",
   "metadata": {},
   "source": [
    "## Challenge Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237298c2-2986-4bba-bcbd-5ff27e361919",
   "metadata": {},
   "source": [
    "The Ekstra Bladet RecSys Challenge aims to predict which article a user will click on from a list of articles that were seen during a specific impression. Utilizing the user's click history, session details (like time and device used), and personal metadata (including gender and age), along with a list of candidate news articles listed in an impression log, the challenge's objective is to rank the candidate articles based on the user's personal preferences. This involves developing models that encapsulate both the users and the articles through their content and the users' interests. The models are to estimate the likelihood of a user clicking on each article by evaluating the compatibility between the article's content and the user's preferences. The articles are ranked based on these likelihood scores, and the precision of these rankings is measured against the actual selections made by users. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788cf29",
   "metadata": {},
   "source": [
    "## Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766e23f",
   "metadata": {},
   "source": [
    "The Ekstra Bladet News Recommendation Dataset (EB-NeRD) was created to support advancements in news recommendation research. It was collected from user behavior logs at Ekstra Bladet. We collected behavior logs from active users during the 6 weeks from April 27 to June 8, 2023. This timeframe was selected to avoid major events, e.g., holidays or elections, that could trigger atypical behavior at Ekstra Bladet. The active users were defined as users who had at least 5 and at most 1,000 news click records in a three-week period from May 18 to June 8, 2023. To protect user privacy, every user was delinked from the production system when securely hashed into an anonymized ID using one-time salt mapping. Alongside, we provide Danish news articles published by Ekstra Bladet. Each article is enriched with textual context features such as title, abstract, body, categories, among others. Furthermore, we provide features that have been generated by proprietary models, including topics, named entity recognition (NER), and article embeddings [2]\n",
    "\n",
    "For more information on the [dataset](https://recsys.eb.dk/dataset/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb08b9d",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [RecySys Challenge 2024 Logistics](https://recsys.eb.dk/)\n",
    "\n",
    "[2] [Ekstra Bladet News Recommendation Dataset](https://recsys.eb.dk/dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e0d1c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97e6cd",
   "metadata": {},
   "source": [
    "### Notebook Organization\n",
    "### This purpose of this notebook is for EDA only. \n",
    "\n",
    "- Logistics\n",
    "- EDA \n",
    "    - Data Preprocessing\n",
    "    - Functions\n",
    "        - Plot Functions\n",
    "        - Feature Functions\n",
    "            - Article\n",
    "            - User\n",
    "            - Topic\n",
    "            - Activity\n",
    "    - Feature Analysis\n",
    "        - Overall Feature Analysis\n",
    "        - Article\n",
    "        - User\n",
    "        - Session\n",
    "        - Topic\n",
    "        - Devices\n",
    "        - If subscriber\n",
    "        - Gender\n",
    "        - Age\n",
    "        - Postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e806296",
   "metadata": {},
   "source": [
    "We need to establish specific metrics and analyze how different features impact those metrics. Our platform generates revenue through both subscriptions and advertisements. User engagement is crucial because the more time users spend reading new articles, the greater our advertisement revenue. With this in focus, let's start with exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927be23c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f05b58-dde3-4d5f-87a2-f20ad71cd62b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4caea",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d0f39",
   "metadata": {},
   "source": [
    "Let's import our packages used for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651173e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616fa6a4-e47a-4116-82f5-896d6893a00e",
   "metadata": {},
   "source": [
    "Load in the three separate data sources of the dataset:\n",
    "\n",
    "**Articles**: Detailed information of news articles.[*](https://recsys.eb.dk/dataset/#articles)\n",
    "\n",
    "**Behaviors**: Impression Logs. [*](https://recsys.eb.dk/dataset/#behaviors)\n",
    "\n",
    "**History**: Click histories of users. [*](https://recsys.eb.dk/dataset/#history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a57c46-4e08-4299-8a69-12d45e274b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in various dataframes\n",
    "# Articles\n",
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/train/history.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8b12076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in various dataframes\n",
    "# Articles\n",
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/validation/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/validation/history.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d447c",
   "metadata": {},
   "source": [
    "What feature can we join the data sources on?\n",
    "\n",
    "- Articles & Behavior: Article ID\n",
    "\n",
    "- History & Behavior: User ID\n",
    "\n",
    "Before we can join, we need to modify the behavior['article_ids_clicked'] column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5eaa9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datatype of column first\n",
    "df_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if isinstance(x, str) else int(x) if not np.isnan(x) else x)\n",
    "\n",
    "# Join bevhaiors to article\n",
    "df = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n",
    "\n",
    "# Join bevhaiors to history\n",
    "df = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n",
    "\n",
    "# Drop all other dataframes from me\n",
    "df_bev = []\n",
    "df_his = []\n",
    "df_art = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87a8d8",
   "metadata": {},
   "source": [
    "More preprocessing needed before we can begin further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3cc9be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_(x):\n",
    "    \"\"\" \n",
    "    Changes the device input from a int to a str\n",
    "    Keyword arguments:\n",
    "        x -- int\n",
    "    Output:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if x == 1:\n",
    "        return 'Desktop'\n",
    "    elif x == 2:\n",
    "        return 'Mobile'\n",
    "    else:\n",
    "        return 'Tablet'\n",
    "\n",
    "def gender_(x):\n",
    "    \"\"\" \n",
    "    Changes the gender input from a float to a str\n",
    "    Keyword arguments:\n",
    "        x -- float\n",
    "    Output:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if x == 0.0:\n",
    "        return 'Male'\n",
    "    elif x == 1.0:\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def postcodes_(x):\n",
    "    \"\"\" \n",
    "    Changes the postcodes input from a float to a str\n",
    "    Keyword arguments:\n",
    "        x -- float\n",
    "    Output:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if x == 0.0:\n",
    "        return 'Metropolitan'\n",
    "    elif x == 1.0:\n",
    "        return 'Rural District'\n",
    "\n",
    "    elif x == 2.0:\n",
    "        return 'Municipality'\n",
    "\n",
    "    elif x == 3.0:\n",
    "        return 'Provincial'\n",
    "\n",
    "    elif x == 4.0:\n",
    "        return 'Big City'\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5796b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df.dropna(subset=['article_id'], inplace=True)\n",
    "\n",
    "# Change article IDs into int\n",
    "df['article_id'] = df['article_id'].apply(lambda x: int(x))\n",
    "df['article_id'] = df['article_id'].astype(np.int64)\n",
    "\n",
    "# Change age from int to string\n",
    "df['device_type'] = df['device_type'].apply(lambda x: device_(x))\n",
    "\n",
    "# Change genders from float to string\n",
    "df['gender'] = df['gender'].apply(lambda x: gender_(x))\n",
    "\n",
    "# Change age to str it's a range\n",
    "df['age'] = df['age'].astype('Int64')\n",
    "df['age'] = df['age'].astype(str)\n",
    "df['age'] = df['age'].apply(\n",
    "    lambda x: x if x == '<NA>' else x + ' - ' + x[0] + '9')\n",
    "\n",
    "\n",
    "# Change postcodes from int to str\n",
    "df['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3427472",
   "metadata": {},
   "source": [
    "Next section will be on all the helper functions used in this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2493e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a4266",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a399c",
   "metadata": {},
   "source": [
    "## Try content based apporach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e4532dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column which has stuff we can compare:\n",
    "## Title, Body, category, article type, NER, entities, topics\n",
    "\n",
    "## so we have to look at the user's ID, figure out what stuff he has looked at. join that stuff all together and then doa  cosine similarity compared to the impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0083018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topics_str'] = df['topics'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ebe9ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_content'] = df['title'] + \" \" + df['body'] + \" \" +  df[\"category_str\"] + \" \" + df['article_type'] + \" \" + df['ner_clusters'] + \" \" + df['entity_groups'] + df['topics_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c0091c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/validation/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/validation/history.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110a7461",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Take the corpus and use tf_idf on it\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     v \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 43\u001b[0m     tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     cosine_similarities \u001b[38;5;241m=\u001b[39m linear_kernel(tfidf_matrix, tfidf_matrix)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Now for each impression get the article information\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1361\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;66;03m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;66;03m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Pre-compute string fields once\n",
    "df_art['topics_str'] = df_art['topics'].apply(lambda x: ' '.join(x))\n",
    "df_art['entity_groups_str'] = df_art['entity_groups'].apply(lambda x: ' '.join(x))\n",
    "df_art['ner_clusters_str'] = df_art['ner_clusters'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a dictionary for quick lookups once\n",
    "article_content_dict = {}\n",
    "for _, row in df_art.iterrows():\n",
    "    full_content = f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} {row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n",
    "    article_content_dict[row['article_id']] = full_content\n",
    "\n",
    "# Iterate over user behaviors and generate the corpus\n",
    "for i in df_bev.index:\n",
    "    # Get user ID (if needed)\n",
    "    user_id = df_bev.loc[i, 'user_id']\n",
    "\n",
    "    # Get previous profile information\n",
    "    user_article_history = df_bev.loc[i, 'article_id']  # Assuming 'article_id\"\n",
    "\n",
    "    # Ensure user_article_history is a list or handle NaNs\n",
    "    if pd.isna(user_article_history):\n",
    "        user_article_history = []\n",
    "    elif isinstance(user_article_history, (int, float)):\n",
    "        user_article_history = [int(user_article_history)]  # Convert to list if it's a single article ID\n",
    "    elif isinstance(user_article_history, str):\n",
    "        user_article_history = [user_article_history]  # Convert to list if it's a single article ID in string form\n",
    "\n",
    "    # Generate the corpus based on user_article_history\n",
    "    corpus = ' '.join([article_content_dict[x] for x in user_article_history if x in article_content_dict])\n",
    "\n",
    "    if corpus == '':\n",
    "        continue\n",
    "    else:\n",
    "        # Take the corpus and use tf_idf on it\n",
    "        v = TfidfVectorizer()\n",
    "        tfidf_matrix = v.fit_transform(corpus)\n",
    "        cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        # Now for each impression get the article information\n",
    "        for imp in df_bev['article_ids_inview'][i]:\n",
    "            content_imp = article_content_dict[imp]\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f229df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "\n",
    "# Pre-compute string fields once\n",
    "df_art['topics_str'] = df_art['topics'].apply(lambda x: ' '.join(x))\n",
    "df_art['entity_groups_str'] = df_art['entity_groups'].apply(lambda x: ' '.join(x))\n",
    "df_art['ner_clusters_str'] = df_art['ner_clusters'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a dictionary for quick lookups once\n",
    "article_content_dict = {}\n",
    "for _, row in df_art.iterrows():\n",
    "    full_content = f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} {row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n",
    "    article_content_dict[row['article_id']] = full_content\n",
    "\n",
    "# Iterate over user behaviors and generate the corpus\n",
    "for i in df_bev.index:\n",
    "    # Get user ID (if needed)\n",
    "    user_id = df_bev.loc[i, 'user_id']\n",
    "\n",
    "    # Get previous profile information\n",
    "    user_article_history = df_bev.loc[i, 'article_id']  # Assuming 'article_id_fixed' is the column containing lists of article IDs\n",
    "\n",
    "    # Ensure user_article_history is a list or handle NaNs\n",
    "    if pd.isna(user_article_history):\n",
    "        user_article_history = []\n",
    "    elif isinstance(user_article_history, (int, float)):\n",
    "        user_article_history = [int(user_article_history)]  # Convert to list if it's a single article ID\n",
    "    elif isinstance(user_article_history, str):\n",
    "        user_article_history = [user_article_history]  # Convert to list if it's a single article ID in string form\n",
    "\n",
    "    # Generate the corpus based on user_article_history\n",
    "    corpus = [article_content_dict[x] for x in user_article_history if x in article_content_dict]\n",
    "\n",
    "    if not corpus:\n",
    "        continue\n",
    "    else:\n",
    "        # Take the corpus and use tf_idf on it\n",
    "        v = TfidfVectorizer()\n",
    "        tfidf_matrix = v.fit_transform(corpus)\n",
    "        cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        # Now for each impression get the article information\n",
    "        for imp in df_bev.loc[i, 'article_ids_inview']:  # Assuming 'article_ids_inview' is the correct column name\n",
    "            if imp in article_content_dict:\n",
    "                content_imp = article_content_dict[imp]\n",
    "                # Do something with content_imp\n",
    "                # For example: print(content_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e97439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>last_modified_time</th>\n",
       "      <th>premium</th>\n",
       "      <th>body</th>\n",
       "      <th>published_time</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>article_type</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>category_str</th>\n",
       "      <th>total_inviews</th>\n",
       "      <th>total_pageviews</th>\n",
       "      <th>total_read_time</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>topics_str</th>\n",
       "      <th>entity_groups_str</th>\n",
       "      <th>ner_clusters_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19270</th>\n",
       "      <td>9782656</td>\n",
       "      <td>Få masser af data i udlandet til latterligt la...</td>\n",
       "      <td>Bruger du meget mobildata, selv når du er på f...</td>\n",
       "      <td>2023-06-29 06:49:06</td>\n",
       "      <td>False</td>\n",
       "      <td>Mange danskere ynder at rejse til varmere himm...</td>\n",
       "      <td>2023-05-27 18:53:26</td>\n",
       "      <td>[9782638, 8428800, 9782641]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/forbrug/Teknologi/faa-...</td>\n",
       "      <td>...</td>\n",
       "      <td>[2865]</td>\n",
       "      <td>forbrug</td>\n",
       "      <td>430776.0</td>\n",
       "      <td>48740.0</td>\n",
       "      <td>2973951.0</td>\n",
       "      <td>0.7073</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Økonomi Mikro Teknologi</td>\n",
       "      <td>LOC MISC MISC ORG</td>\n",
       "      <td>Danmark dansker danskere EU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id                                              title  \\\n",
       "19270     9782656  Få masser af data i udlandet til latterligt la...   \n",
       "\n",
       "                                                subtitle  last_modified_time  \\\n",
       "19270  Bruger du meget mobildata, selv når du er på f... 2023-06-29 06:49:06   \n",
       "\n",
       "       premium                                               body  \\\n",
       "19270    False  Mange danskere ynder at rejse til varmere himm...   \n",
       "\n",
       "           published_time                    image_ids     article_type  \\\n",
       "19270 2023-05-27 18:53:26  [9782638, 8428800, 9782641]  article_default   \n",
       "\n",
       "                                                     url  ... subcategory  \\\n",
       "19270  https://ekstrabladet.dk/forbrug/Teknologi/faa-...  ...      [2865]   \n",
       "\n",
       "      category_str total_inviews  total_pageviews total_read_time  \\\n",
       "19270      forbrug      430776.0          48740.0       2973951.0   \n",
       "\n",
       "      sentiment_score  sentiment_label               topics_str  \\\n",
       "19270          0.7073          Neutral  Økonomi Mikro Teknologi   \n",
       "\n",
       "       entity_groups_str             ner_clusters_str  \n",
       "19270  LOC MISC MISC ORG  Danmark dansker danskere EU  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_art[df_art['article_id'] == 9782656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e728c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "\n",
    "# Pre-compute string fields once\n",
    "df_art['topics_str'] = df_art['topics'].apply(lambda x: ' '.join(x))\n",
    "df_art['entity_groups_str'] = df_art['entity_groups'].apply(lambda x: ' '.join(x))\n",
    "df_art['ner_clusters_str'] = df_art['ner_clusters'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a dictionary for quick lookups once\n",
    "article_content_dict = {}\n",
    "for _, row in df_art.iterrows():\n",
    "    full_content = f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} {row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n",
    "    article_content_dict[row['article_id']] = full_content\n",
    "\n",
    "# Iterate over user behaviors and generate the corpus\n",
    "for i in df_bev.index:\n",
    "    # Get user ID (if needed)\n",
    "    user_id = df_bev.loc[i, 'user_id']\n",
    "\n",
    "    # Get previous profile information\n",
    "    user_article_history = df_bev.loc[i, 'article_id']  # Assuming 'article_id' is the column containing lists of article IDs\n",
    "\n",
    "    # Ensure user_article_history is a list or handle NaNs\n",
    "    if pd.isna(user_article_history):\n",
    "        user_article_history = []\n",
    "    elif isinstance(user_article_history, (int, float)):\n",
    "        user_article_history = [int(user_article_history)]  # Convert to list if it's a single article ID\n",
    "    elif isinstance(user_article_history, str):\n",
    "        user_article_history = [user_article_history]  # Convert to list if it's a single article ID in string form\n",
    "    elif isinstance(user_article_history, list):\n",
    "        user_article_history = [int(x) if isinstance(x, (int, float)) else x for x in user_article_history]\n",
    "\n",
    "    # Generate the corpus based on user_article_history\n",
    "    corpus = [article_content_dict[x] for x in user_article_history if x in article_content_dict]\n",
    "\n",
    "    # Skip processing if the corpus is empty\n",
    "    if not corpus:\n",
    "        continue\n",
    "\n",
    "    # Take the corpus and use tf-idf on it\n",
    "    v = TfidfVectorizer()\n",
    "    tfidf_matrix = v.fit_transform(corpus)\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Now for each impression get the article information\n",
    "    for imp in df_bev.loc[i, 'article_ids_inview']:  # Assuming 'article_ids_inview' is the correct column name\n",
    "        if imp in article_content_dict:\n",
    "            content_imp = article_content_dict[imp]\n",
    "            # Do something with content_imp\n",
    "            # For example: print(content_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77e955ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m content_imp \u001b[38;5;241m=\u001b[39m article_content_dict[imp]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Compute TF-IDF for the content_imp\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m tfidf_imp \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontent_imp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity between the corpus and the content_imp\u001b[39;00m\n\u001b[0;32m     57\u001b[0m similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(tfidf_matrix, tfidf_imp)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2151\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2148\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2150\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m-> 2151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1721\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1718\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m, attributes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midf_\u001b[39m\u001b[38;5;124m\"\u001b[39m], msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midf vector is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;66;03m# *= doesn't work\u001b[39;00m\n\u001b[1;32m-> 1721\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_idf_diag\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1724\u001b[0m     X \u001b[38;5;241m=\u001b[39m normalize(X, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_matrix.py:48\u001b[0m, in \u001b[0;36mspmatrix.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:535\u001b[0m, in \u001b[0;36m_spbase._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_sparse_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# If it's a list or whatever, treat it like an array\u001b[39;00m\n\u001b[0;32m    538\u001b[0m other_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(other)\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:517\u001b[0m, in \u001b[0;36m_cs_matrix._mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    513\u001b[0m idx_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_dtype((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[0;32m    514\u001b[0m                              other\u001b[38;5;241m.\u001b[39mindptr, other\u001b[38;5;241m.\u001b[39mindices))\n\u001b[0;32m    516\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matmat_maxnnz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 517\u001b[0m nnz \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m idx_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_dtype((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[0;32m    524\u001b[0m                              other\u001b[38;5;241m.\u001b[39mindptr, other\u001b[38;5;241m.\u001b[39mindices),\n\u001b[0;32m    525\u001b[0m                             maxval\u001b[38;5;241m=\u001b[39mnnz)\n\u001b[0;32m    527\u001b[0m indptr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(major_axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Pre-compute string fields once\n",
    "df_art['topics_str'] = df_art['topics'].apply(lambda x: ' '.join(x))\n",
    "df_art['entity_groups_str'] = df_art['entity_groups'].apply(lambda x: ' '.join(x))\n",
    "df_art['ner_clusters_str'] = df_art['ner_clusters'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a dictionary for quick lookups once\n",
    "article_content_dict = {}\n",
    "for _, row in df_art.iterrows():\n",
    "    full_content = f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} {row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n",
    "    article_content_dict[row['article_id']] = full_content\n",
    "\n",
    "predicted_impression = []\n",
    "# Iterate over user behaviors and generate the corpus\n",
    "for i in df_bev.index:\n",
    "    # Get previous profile information\n",
    "    user_article_history = df_bev.loc[i, 'article_id']  # Assuming 'article_id' is the column containing lists of article IDs\n",
    "\n",
    "    # Ensure user_article_history is a list or handle NaNs\n",
    "    if pd.isna(user_article_history):\n",
    "        user_article_history = []\n",
    "    elif isinstance(user_article_history, (int, float)):\n",
    "        user_article_history = [int(user_article_history)]  # Convert to list if it's a single article ID\n",
    "    elif isinstance(user_article_history, str):\n",
    "        user_article_history = [user_article_history]  # Convert to list if it's a single article ID in string form\n",
    "    elif isinstance(user_article_history, list):\n",
    "        user_article_history = [int(x) if isinstance(x, (int, float)) else x for x in user_article_history]\n",
    "\n",
    "    # Generate the corpus based on user_article_history\n",
    "    corpus = [article_content_dict[x] for x in user_article_history if x in article_content_dict]\n",
    "\n",
    "    # Skip processing if the corpus is empty\n",
    "    if not corpus:\n",
    "        continue\n",
    "\n",
    "    # Take the corpus and use TF-IDF on it\n",
    "    v = TfidfVectorizer()\n",
    "    tfidf_matrix = v.fit_transform(corpus)\n",
    "\n",
    "    highest_similarity = 0\n",
    "    best_imp = None\n",
    "\n",
    "    # Now for each impression get the article information\n",
    "    for imp in df_bev.loc[i, 'article_ids_inview']:  # Assuming 'article_ids_inview' is the correct column name\n",
    "        if imp in article_content_dict:\n",
    "            content_imp = article_content_dict[imp]\n",
    "\n",
    "            # Compute TF-IDF for the content_imp\n",
    "            tfidf_imp = v.transform([content_imp])\n",
    "\n",
    "            # Calculate cosine similarity between the corpus and the content_imp\n",
    "            similarity = cosine_similarity(tfidf_matrix, tfidf_imp).mean()\n",
    "\n",
    "            # Update highest similarity and best impression\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                best_imp = imp\n",
    "    \n",
    "    predicted_impression.append(best_imp)\n",
    "    #if best_imp is not None:\n",
    "        \n",
    "       # print(f\"Best impression for user {df_bev.loc[i, 'user_id']}: {best_imp} with similarity {highest_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d16519ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Use parallel processing to speed up the computation\u001b[39;00m\n\u001b[0;32m     65\u001b[0m num_cores \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[1;32m---> 66\u001b[0m predicted_impression \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_user\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_bev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Add the predicted impressions back to the dataframe if needed\u001b[39;00m\n\u001b[0;32m     69\u001b[0m df_bev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_impression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_impression\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Combine the preprocessing of string fields and article content dictionary creation\n",
    "df_art['full_content'] = (\n",
    "    df_art['title'] + ' ' + df_art['body'] + ' ' + \n",
    "    df_art['category_str'] + ' ' + df_art['article_type'] + ' ' +\n",
    "    df_art['ner_clusters_str'] + ' ' + df_art['entity_groups_str'] + ' ' +\n",
    "    df_art['topics_str']\n",
    ")\n",
    "article_content_dict = df_art.set_index('article_id')['full_content'].to_dict()\n",
    "\n",
    "# Fit the TF-IDF vectorizer on all articles\n",
    "all_articles = list(article_content_dict.values())\n",
    "v = TfidfVectorizer()\n",
    "v.fit(all_articles)\n",
    "\n",
    "# Transform all article contents once\n",
    "tfidf_all_articles = v.transform(all_articles)\n",
    "article_id_to_index = {article_id: idx for idx, article_id in enumerate(article_content_dict)}\n",
    "\n",
    "def process_user(i):\n",
    "    user_article_history = df_bev.loc[i, 'article_id']\n",
    "\n",
    "    # Ensure user_article_history is a list\n",
    "    if pd.isna(user_article_history):\n",
    "        user_article_history = []\n",
    "    elif isinstance(user_article_history, (int, float)):\n",
    "        user_article_history = [int(user_article_history)]\n",
    "    elif isinstance(user_article_history, str):\n",
    "        user_article_history = [user_article_history]\n",
    "    elif isinstance(user_article_history, list):\n",
    "        user_article_history = [int(x) if isinstance(x, (int, float)) else x for x in user_article_history]\n",
    "\n",
    "    # Generate the TF-IDF matrix for user_article_history\n",
    "    indices = [article_id_to_index[x] for x in user_article_history if x in article_id_to_index]\n",
    "    if not indices:\n",
    "        return None\n",
    "\n",
    "    tfidf_matrix = tfidf_all_articles[indices, :]\n",
    "\n",
    "    highest_similarity = 0\n",
    "    best_imp = None\n",
    "\n",
    "    # Calculate similarity for each impression\n",
    "    for imp in df_bev.loc[i, 'article_ids_inview']:\n",
    "        if imp in article_id_to_index:\n",
    "            content_idx = article_id_to_index[imp]\n",
    "            tfidf_imp = tfidf_all_articles[content_idx, :]\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(tfidf_matrix, tfidf_imp).mean()\n",
    "\n",
    "            # Update highest similarity and best impression\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                best_imp = imp\n",
    "\n",
    "    return best_imp\n",
    "\n",
    "# Use parallel processing to speed up the computation\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "predicted_impression = Parallel(n_jobs=num_cores)(delayed(process_user)(i) for i in df_bev.index)\n",
    "\n",
    "# Add the predicted impressions back to the dataframe if needed\n",
    "df_bev['predicted_impression'] = predicted_impression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c301240f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Få masser af data i udlandet til latterligt lav pris Mange danskere ynder at rejse til varmere himmelstrøg i løbet af sommerferien, og for de flestes vedkommende går turen til et EU-land.\\nEr du en af mange, der skal rejse til inden for EU’s grænser i løbet af sommeren, er der rigtig gode muligheder for at spare penge på mobilregningen.\\nSe også:\\nSammenlign priser på mobilabonnementer\\n59 kroner per måned henover sommeren\\nLavprisselskabet duka frister lige nu med et tilbud\\n, der er svært at modstå. For bare 59 kroner per måned frem til den 30. september 2023 kan du få fri tale i og til EU, 35 GB data til brug i Danmark og yderligere 35 GB til brug i EU. Derefter fortsætter abonnementet til normalprisen på 119 kroner per måned.\\n35 GB data er mere end rigeligt til at dække en gennemsnitglig danskernes dataforbrug, da\\nen dansker i gennemsnit bruger 20,0 GB per måned\\n. Og med 35 GB ekstra til brug i EU-lande er der altså ingen grund til at holde igen med mobilforbruget, bare fordi du er udenlands.\\nduka frister også med et andet abonnement til præcis samme pris. Her får du 100 GB data til brug i Danmark og yderligere 20 GB til brug i EU. Dette abonnement er til dig, som har et højere forbrug herhjemme i Danmark og ikke har brug for nær så meget data under ferien.\\nLæs også:\\nSådan får du bedre dækning på mobilen\\nHusk at give dit abonnement et sommertjek\\nInden du rejser sydpå, eller et hvilken som helst andet sted uden for landets grænser denne sommer, bør du tjekke, om dit mobilabonnement er lige så rejseklar som dig selv.\\nDanskernes evigt stigende forbrug af mobildata betyder, at du højst sandsynligt også har brug for data til rådighed i udlandet. Inden du rejser på ferie bør du derfor sikre dig, at dit mobilabonnement er med EU-data inkluderet. dukas mobilabonnementer er gode bud på billige mobilabonnementer med rigtigt meget data til brug i udlandet.\\nMen der findes også mange andre gode og billige abonnementer. Læs meget mere om, hvordan du og din smartphone bliver rejseklar\\nher\\n.\\nSådan sikrer du dig, når du handler forbrug article_default Danmark dansker danskere EU LOC MISC MISC ORG Økonomi Mikro Teknologi'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_content_dict[9782656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9578f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf02c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9783865, 9784591, 9784679, 9784696, 9784710])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bev['article_ids_inview'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a6409d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_content_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43marticle_content_dict\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'article_content_dict' is not defined"
     ]
    }
   ],
   "source": [
    "article_content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac9b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n",
    "\n",
    "# Behaviors\n",
    "df_bev = pd.read_parquet(\"Data/Small/validation/behaviors.parquet\")\n",
    "\n",
    "# History\n",
    "df_his = pd.read_parquet(\"Data/Small/validation/history.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c874b814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['impression_id', 'article_id', 'impression_time', 'read_time',\n",
       "       'scroll_percentage', 'device_type', 'article_ids_inview',\n",
       "       'article_ids_clicked', 'user_id', 'is_sso_user', 'gender', 'postcode',\n",
       "       'age', 'is_subscriber', 'session_id', 'next_read_time',\n",
       "       'next_scroll_percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98b67174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'impression_time_fixed', 'scroll_percentage_fixed',\n",
       "       'article_id_fixed', 'read_time_fixed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_his.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc9445ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'title', 'subtitle', 'last_modified_time', 'premium',\n",
       "       'body', 'published_time', 'image_ids', 'article_type', 'url',\n",
       "       'ner_clusters', 'entity_groups', 'topics', 'category', 'subcategory',\n",
       "       'category_str', 'total_inviews', 'total_pageviews', 'total_read_time',\n",
       "       'sentiment_score', 'sentiment_label', 'topics_str', 'entity_groups_str',\n",
       "       'ner_clusters_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_art.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9fad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
